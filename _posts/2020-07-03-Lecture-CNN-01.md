---
title:  "[DL] CNN_1.개념 (확장)"
excerpt: "<<Neural Network>> CNN 모델 주요 개념을 이해해 보자."
toc: true
toc_sticky: true
categories:
  - Lecture
tags:
  - DL
  - CNN
use_math: true
last_modified_at: 2020-07-13
---



<sup> 출처가 명시되지 않은 모든 자료(이미지 등)는 [조성현 강사님](https://blog.naver.com/chunjein)의 강의 및 강의 자료를 기반으로 합니다.</sup> <sup>[Github Repo](https://github.com/sirzzang/LECTURE/tree/master/인공지능-자연어처리(NLP)-기반-기업-데이터-분석/조성현 강사님/DL/DL 실습/20200703)</sup>

<sup>Tensorflow : 2.2.0</sup>

# _CNN 모델 개념_



 [CNN 모델의 주요 개념](https://sirzzang.github.io/lecture/Lecture-Tensorflow-CNN-concept/){: .btn .btn--danger .btn--small} 은 이전에 문쌤이랑 공부할 때 한 번 훑어 보았기 때문에, 추가하여 배운 내용을 위주로 CNN을 다시 이해해 보자.



## 1. 주요 개념 추가



* `Convolution`을 통해 생성된 `Feature Map`은 입력 데이터(이미지) `X`와 가중치 행렬 `W` 간의 **내적**과 같은 개념이~~(라고 한)~~다. 두 벡터의 내적은 두 벡터의 **상관성**과 밀접한 관계가 있으므로, 서로 겹치는 부분일수록 Convolution 값이 **크고**, 이미지와 필터 간 **상관성이 높다**.
* 알고 보니, `Convolution`은 `Filter`가 뒤집힌 상태를 적용한다. Tensorflow의 Convolution은 계산의 용이성을 위해 `Cross-Correlation` 방식을 사용한다고 한다(...!). 계산 원리 자체는 비슷한 듯하다.



![image-20200716133841614]({{site.url}}/assets/images/image-20200716133841614.png){: .width="600"}{:. align-center}

<br>

* `Pooling layer`은 `non-trainable layer`로서 down sampling을 하기 위한 크기만 지정한다. 데이터 소실의 우려도 있지만, 반대로 입력 데이터의 특정 부분이 약간 틀어지거나 위치가 변하더라도 *동일한* 결과를 얻기 위해 사용한다. 고양이 코의 위치가 달라져도 고양이 코라고 인식할 수 있다. 이미지의 일부분이 변형되더라도 그 특징을 통해 정상적으로 인식할 수 있는, **시각 불변성 특성**을 보존하기 위해서이다.
* `Feature Map`의 벡터들은 특징이 추출되어 원본 데이터의 잠재된 특징을 내재하고 있는 `Latent Feature`로 볼 수 있다.



## 2. CNN 네트워크와 데이터 구조



![image-20200716135039164]({{site.url}}/assets/images/image-20200716135039164.png){: .width="600"}{:. align-center}



 Tensorflow에 구현되어 있는 Convolution 함수를 보면 `1D`, `2D`, `3D` 등 차원이 붙어 있다. 이는 `Filter`가 이동하는 차원을 나타낸다. `Filter`의 이동에 따라 데이터의 shape을 맞춰 주는 일이 매우 중요하다.



* 1D Convolution : `(batch, time-step, feature)`

 `Filter`가 한 방향으로 이동한다. 시계열, 음성, 문장 등 한 방향으로 이동하면 되는 데이터의 경우 사용한다. 지난 시간에 배운 RNN, LSTM 등과 동일한 구조이다. 입력 데이터의 shape은 3차원이다. 



* 2D Convolution : `(batch, row, column, channel)`

 `Filter`가 두 방향으로 이동한다. 이미지 한 장 등과 같은 데이터에 사용한다.  입력 데이터의 shape은 4차원이다.



* 3D Convolution : `(batch, dim1, dim2, dim3, channel)`

 `Filter`가 세 방향으로 이동한다. 동영상 한 부분, 이미지 여러 장 등과 같은 데이터에 적용한다. 필터의 이동 방향에 time 축이 추가된다. 참고로, 컬러 이미지의 경우는 필터의 채널이 3일 뿐, 세 방향으로 이동하는 것이 아니다. 입력 데이터의 shape은 5차원이다.



## 3. 오류 역전파 알고리즘



 일반적인 다중 레이어 인공신경망의 back propagation 알고리즘과 크게 다르지 않다. 아래 그림과 같이 3x3 이미지 데이터에 2x2 필터를 적용해 2x2 `Feature Map`을 생성하는 네트워크를 생각해 보자.



![image-20200716140519522]({{site.url}}/assets/images/image-20200716140519522.png){: width="400" height="400"}{: .align-center}



 `Convolution`하여 `Feature Map`을 생성해 내는 과정 자체를 레이어 간 connection으로 보고, 최종 output layer에서의 에러에 chain rule을 적용하여 `Convolution Layer`까지 에러를 역전파한다. 





## 4. Upsampling



 이미지의 차원을 줄이는 것이 아니라, 반대로 늘려주는 방법이다. `Transposed Convolution`이라고도 한다. 고정된 값 필터를 사용할 수도 있고(이 경우, 해당 레이어는 non-trainable layer가 된다.), `Convolution` 레이어처럼 weight와 bias를 설정하여 학습 가능하게 만들 수도 있다.

 이전까지의 `Convolution`, `Pooling` 등이 모두 차원을 줄여 특정 부분의 feature를 강조하고, 계산 속도를 빠르게 하기 위한 Downsampling이었다면, Upsampling은 원래 이미지 크기로 복원하고 싶은 경우 사용한다. **단, 이미 소실된 feature나 유실된 데이터를 복원할 수는 없다.** 이후 배울 AutoEncoder 모델에서 유용하게 사용될 것이므로 그 작동 원리를 잘 기억해 두자.



![image-20200716141546360]({{site.url}}/assets/images/image-20200716141546360.png){: .width="600"}{:. align-center}





 아래 엑셀 실습 파일의 이미지를 통해, upsampling을 거쳤음에도 불구하고 원래의 입력 이미지가 변형되었음을 확인하자.



![image-20200716141805047]({{site.url}}/assets/images/image-20200716141805047.png){: .width="600"}{:. align-center}



**0713 추가 : Conv2DTranspose 작동 원리**



 오토 인코더 강의 때 집단 지성으로 Tensorflow에서 `Transposed Convolution`이 어떻게 작동하는지 알아 냈다!  [유레카!](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose){: .btn .btn--success}  



 2차원 `Conv2DTranspose`를 예로 들어 결과가 어떻게 도출되는지 살펴보자. 중점적으로 알아보고자 하는 것은 다음의 2가지이다.

* stride 크기의 의미
* padding 옵션과 stride 크기의 관계



 

 차원이 축소된 원래의 이미지에 `3 x 3` 크기의 1로 구성된 고정 값 필터를 적용해 upsampling한다고 하는 상황을 가정하자. 해당 필터를 원래 이미지 한 칸씩을 적용해 가며 map (*~~정확한 용어를 모르겠다~~*) 을 만든다.



![stage1]({{site.url}}/assets/images/conv2d-transpose-1.png){: width="500" height="300"}{: .align-center}



 이제 이 상태에서 `stride` 크기를 2로 한다. 각각의 map을 1번 map, 2번 map, 3번 map, 4번 map이라고 한다. 1번 map에서 2칸을 이동해 2번 map을 겹치고, 아래 쪽으로 내려가 1번 map에서 2칸을 이동해 3번 map을 겹친다. 이제 3번 map에서 오른쪽으로 2칸을 이동해 4번 map을 겹친다. 그러면 아래와 같이 가운데 십자가 부분이 겹친 map을 얻을 수 있다.



![stage2]({{site.url}}/assets/images/conv2d-transpose-2.png){: width="500" height="300"}{: .align-center}



 여기서 padding으로 `SAME` 옵션을 주면 transpose 결과 이미지가 원본 이미지의 `stride` 배가 되라고 하는 의미이다. 따라서 결과 이미지가 `4 x 4` 크기가 되도록 이미지를 끝에서부터 자르고, 겹친 부분은 더한다.

 반면 padding으로 `VALID` 옵션을 주면, transpose 결과 이미지를 그대로 두라는 의미이다. 가운데 겹쳐진 숫자를 더하는 것은 마찬가지이다. 



 만약 `stride`를 3으로 준다면 어차피 각각의 필터를 적용해 map을 적용한 뒤 3칸을 이동하기 때문에 겹치는 부분이 없을 것이다. 따라서 패딩 옵션에 상관 없이 동일한 결과가 나온다.