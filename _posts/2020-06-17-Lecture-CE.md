---
title:  "[ML/DL] Cross Entropy"
excerpt: "섀넌 엔트로피, 크로스 엔트로피, 손실 함수."
toc: true
toc_sticky: true
categories:
  - Lecture
tags:
  - ML
  - DL
  - Entropy
use_math: true
last_modified_at: 2020-06-17
---



<sup>출처가 명시되지 않은 모든 자료(이미지 등)는 [조성현 강사님](https://blog.naver.com/chunjein)의 강의 및 강의 자료를 기반으로 합니다.</sup>

<sup>[Github Repo](https://github.com/sirzzang/LECTURE/blob/master/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC(NLP)-%EA%B8%B0%EB%B0%98-%EA%B8%B0%EC%97%85-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D/%EC%A1%B0%EC%84%B1%ED%98%84%20%EA%B0%95%EC%82%AC%EB%8B%98/%5B20200617%5D%20CrossEntropy.pdf)</sup>

# Cross Entropy





## 1. 정보 엔트로피



 정보 이론에서의 Entropy는 1948년 Claude Shannon이 제안한 개념으로, 다음의 아이디어를 반영한 개념이다.

* 확률이 큰 사건일수록 정보량은 작다.
* 독립적인 두 사건에서 얻을 수 있는 정보량은 두 사건에서 얻을 수 있는 정보량의 합이다.
* 정보량은 비트로 표현된다.



 이 아이디어를 모두 반영하기 위해서 Shannon은 어떤 사건에서 얻을 수 있는 정보의 양을 해당 사건의 로그 확률로 정의했다. 그리고 어떠한 확률 분포에서 얻을 수 있는 정보량을 확률 이론의 기댓값 계산 방식에 따라 기댓값으로 나타냈다.



![entropy]({{site.url}}/assets/images/entropy.png)



>  *참고*
>
>  교재에 명시되어 있는 정보 엔트로피 식은 정보량이 비트로 표현된다는 아이디어를 반영하지 않은 식이다. 이 부분은 강의에서 진행하지 않았으나, 추후 검색을 통해 찾아 반영해 작성한 내용인데, 원래 Shannon의 정보 엔트로피 식에서는 로그 확률의 밑이 2로 표현된다.
>
>  강사님께서는 정보 엔트로피와 열역학 엔트로피 사이의 연관 관계 및 그로부터 얻을 수 있는 정보 가치에 더 중점을 두어 설명하신 듯하다.



 결국 정보 이론에서의 엔트로피는 정보의 불확실성에 대한 척도를 의미한다. 머신러닝은 모델링을 통해 불확실한 정보를 줄이고 실제 정보를 예측하고자 하는 작업이다. Decision Tree 수업 내용 및 아래 Udacity 강의 내용을 참고하자.



|                           Entropy                            |                           Impurity                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![entropy_udacity1]({{site.url}}/assets/images/udacity1.png) | ![entropy-udacity2]({{site.url}}/assets/images/udacity2.png) |

<center><sup> https://www.youtube.com/watch?v=NHAatuG0T3Q </sup></center>







## 2. 크로스 엔트로피



 서로 다른 확률 분포 간의 정보 엔트로피이다. 두 확률 분포를 교차로 곱해 정보량을 계산한다는 의미에서 '크로스'란 이름이 붙는 듯하다.



![cross entropy]({{site.url}}/assets/images/image-20200623100124725.png)

 엔트로피 식에서 정보 가치를 정의하는 부분이 달라진다. 위의 식에 의하면, 크로스 엔트로피는 두 분포 사이의 *차이* 및 여기서 오는 **불확실성**을 계산하는 방식이다.



 머신러닝, 딥러닝에서 분류 문제의 손실 함수로 주로 사용된다. 이 때 예측 분포가 Q, 실제 분포가 P가 된다.  

* 실제 확률 분포를 모르는 상태에서, 
* 학습 데이터를 바탕으로 미래의 확률 분포를 예측하기 위한 모델을 만들고, 
* 예측한 분포에 따라 획득한 정보가 얼마나 유용한지 그 정보량을 계산한다.



 머신 러닝을 통해 예측 모형을 만들 때, 훈련 데이터에 대해서는 실제 분포 P를 알 수 있기 때문에, 예측 모델에서의 확률 분포 Q와 실제 분포 P 간의 크로스 엔트로피 값을 계산할 수 있다. 이렇게 계산된 크로스 엔트로피 값은 실제 값과 모델이 예측한 값 사이의 정보 불확실성을 나타낸다. 따라서 손실함수로서 크로스 엔트로피를 사용하여 **불확실성을 줄여 나가는 방향으로** 학습이 이루어지게 된다.





### 크로스 엔트로피 계산 예시



![iris classification]({{site.url}}/assets/images/iris.png)



 iris dataset을 분류하는 유명한 문제를 생각해 보자.

 분류해야 하는 꽃의 품종이 Versicolor라고 하자. 그렇다면 실제 관측값에서 꽃이 Versicolor일 확률은 1, Setosa일 확률은 0, Verginica일 확률은 0이 된다. 편의상 각 클래스를 0, 1, 2로 하여 실제 관측값의 확률을 나타내면 다음과 같다.
$$
y^t = [1 \ 0 \ 0]
$$


 머신러닝을 통해 모델링을 진행한 결과, 품종을 예측하기 위해 Q라는 확률 분포를 만들어 냈다고 하자. 이 때, 기계가 해당 확률 분포를 바탕으로 품종을 완전히 다르게 예측한 경우, 비슷하게 예측한 경우, 완전히 맞게 예측한 경우 Cross Entropy를 계산해 보자.



* 완전히 틀린 경우


$$
y^t = [1.0 \ 0.0 \ 0.0] \\
y^p = [0.0 \ 1.0 \ 0.0] \\
CE = log\frac {1} {0.0} \times 1 + log\frac {1} {1.0} \times 0 + log\frac {1} {0.0} \times 0 = \inf
$$


* 어느 정도 맞은 경우

  

   기계가 데이터를 보고, 해당 데이터가 Virginica일 확률을 0.7, Setosa일 확률을 0.1, Versicolor일 확률을 0.2라고 예측했다고 하자.


$$
  y^t = [1.0 \ 0.0 \ 0.0] \\
  y^p = [0.7 \ 0.1 \ 0.2] \\
  CE = log\frac {1} {0.7} \times 1 + log\frac {1} {0.1} \times 0 + log\frac {1} {0.2} \times 0 = 0.35667494393
$$



* 완전히 맞은 경우

$$
y^t = [1.0 \ 0.0 \ 0.0] \\
y^p = [1.0 \ 0.0 \ 0.0] \\
CE = log\frac {1} {1.0} \times 1 + log\frac {1} {0.0} \times 0 + log\frac {1} {0.0} \times 0 = 0
$$





### 손실함수



 위의 계산 예시를 통해 크로스 엔트로피의 성질을 알 수 있다.



 우선, 크로스 엔트로피 값은 항상 엔트로피보다 크거나 같다. 직관적으로 이해하면, 크로스 엔트로피 값은 예측된 값을 바탕으로 실제 정보량을 계산하기 때문에, 완전히 맞게 예측하지 않는 한 틀린 정보를 가지고 있을 수밖에 없다. 따라서 실제 맞는 정보의 양보다 더 많은 정보를 가질 수밖에 없다. 

 둘째, 예측된 분포가 실제 분포와 비슷해질 수록 크로스 엔트로피 값은 엔트로피 값과 가까워 진다. 

 

 특히 두 번째 성질은 크로스 엔트로피가 머신러닝의 손실 함수로 사용될 수 있는 것임을 알려 준다. 결국 머신러닝의 학습이 잘 진행될수록 크로스 엔트로피 값과 엔트로피 값이 비슷해지기 때문에, 이 값을 계산해서 학습 과정을 조정하면 머신러닝이 예측하는 확률 분포를 실제 확률 분포에 가깝게 만들 수 있는 것이다.





 기존에 배웠던 손실함수인 MSE와 비교해서 살펴 보면, 모델의 정확도가 상승할수록 CE와 MSE가 모두 감소하는 것을 알 수 있다. 

![image-20200623112106497]({{site.url}}/assets/images/image-20200623112106497.png)



 그리고 일반적으로 분류 문제에서는 MSE보다 CE를 사용하는 것이 더 좋다고 알려져 있다(이 부분에 대해서는 [이 블로그](https://ratsgo.github.io/deep%20learning/2017/09/24/loss/)를 참고하자).

