---
title:  "[ML/DL] Cross Entropy"
excerpt: "크로스 엔트로피, 손실 함수."
toc: true
toc_sticky: true
categories:
  - Lecture
tags:
  - ML
  - DL
  - Entropy
use_math: true
last_modified_at: 2020-06-17
---



<sup>출처가 명시되지 않은 모든 자료(이미지 등)는 [조성현 강사님](https://blog.naver.com/chunjein)의 강의 및 강의 자료를 기반으로 합니다. </sup> <sup>[Github Repo](https://github.com/sirzzang/LECTURE/blob/master/인공지능-자연어처리(NLP)-기반-기업-데이터-분석/조성현 강사님/ML/[20200617] CrossEntropy.pdf)</sup>

# _Cross Entropy_





## 1. 개념



 서로 다른 확률 분포 간의 정보 엔트로피이다. 두 확률 분포를 교차로 곱해 정보량을 계산한다는 의미에서 '크로스'란 이름이 붙는 듯하다.



![cross entropy]({{site.url}}/assets/images/image-20200623100124725.png)



 엔트로피 식에서 정보량을 정의하는 부분이 달라진다. 갑자기 `p` 자리에 `q`가 들어 왔다.

 본래 엔트로피가 (특정 확률 분포에서 오는) 정보의 불확실성을 나타낸다고 하였다. 따라서 위의 식에 따르면, 크로스 엔트로피는 두 분포 `P`, `Q`의 차이 및 여기서 오는 정보의 불확실성이 된다.



## 2. 의미



 기존의 엔트로피 식과 크로스 엔트로피 식을 조금 더 자세히 비교해 보자.


$$
H(x) = \Sigma_{i=1}^n log(\frac {1} {p_i}) * pi \\
H(P, Q) = \Sigma_{i=1}^n log(\frac {1} {q_i}) * pi \\
$$


 엔트로피 식에서 `log`가 붙지 않은 부분이 실제 확률을 나타내고, `log`가 붙은 부분이 확률 분포가 나타내는 정보의 양을 나타낸다. 따라서 실제 확률 분포가 `P`일 때, 그 확률 분포로부터 얻을 수 있는 정보의 양을 나타낸 것이 `엔트로피`라면, 실제 확률 분포가 `P`일 때, 그와는 *다른*  어떠한 확률 분포 `Q`로부터 얻을 수 있는 정보의 양을 나타낸 것이 `크로스 엔트로피`가 된다.

 이러한 정의 때문에, 크로스 엔트로피는 주로 *틀릴 수 있는 분포*에서 얻을 수 있는 정보의 양을 계산하는 데에 사용된다. 더 엄밀히 말하면, 실제 분포 `P`를 예측한 틀릴 수 있는 확률 분포 `Q`가 가지고 있는 정보량 *혹은* 불확실성을 나타낸다.





### 2.1. 예시



 iris dataset을 분류하는 유명한 문제를 생각해 보자.

 분류해야 하는 꽃의 품종이 Versicolor라고 하자. 그렇다면 실제 관측값에서 꽃이 Versicolor일 확률은 1, Setosa일 확률은 0, Verginica일 확률은 0이 된다. 편의상 각 클래스를 0, 1, 2로 하여 실제 관측값의 확률을 나타내면 다음과 같다.
$$
y^t = [1 \ 0 \ 0]
$$


 머신러닝을 통해 모델링을 진행한 결과, 품종을 예측하기 위해 Q라는 확률 분포를 만들어 냈다고 하자. 이 때, 기계가 해당 확률 분포를 바탕으로 품종을 완전히 다르게 예측한 경우, 비슷하게 예측한 경우, 완전히 맞게 예측한 경우 Cross Entropy를 계산해 보자.



* 완전히 틀린 경우


$$
y^t = [1.0 \ 0.0 \ 0.0] \\
y^p = [0.0 \ 1.0 \ 0.0] \\
CE = log\frac {1} {0.0} \times 1 + log\frac {1} {1.0} \times 0 + log\frac {1} {0.0} \times 0 = \inf
$$


* 어느 정도 맞은 경우

  

  기계가 데이터를 보고, 해당 데이터가 Virginica일 확률을 0.7, Setosa일 확률을 0.1, Versicolor일 확률을 0.2라고 예측했다고 하자.


$$
  y^t = [1.0 \ 0.0 \ 0.0] \\
  y^p = [0.7 \ 0.1 \ 0.2] \\
  CE = log\frac {1} {0.7} \times 1 + log\frac {1} {0.1} \times 0 + log\frac {1} {0.2} \times 0 = 0.35667494393
$$



* 완전히 맞은 경우

$$
y^t = [1.0 \ 0.0 \ 0.0] \\
y^p = [1.0 \ 0.0 \ 0.0] \\
CE = log\frac {1} {1.0} \times 1 + log\frac {1} {0.0} \times 0 + log\frac {1} {0.0} \times 0 = 0
$$





### 2.2. 특징



 위의 계산 예시를 통해 크로스 엔트로피의 성질을 알 수 있다.



 첫째, 크로스 엔트로피 값은 항상 엔트로피보다 크거나 같다. 직관적으로 이해하면, 크로스 엔트로피 값은 예측된 값을 바탕으로 실제 정보량을 계산하기 때문에, 완전히 맞게 예측하지 않는 한 틀린 정보를 가지고 있을 수밖에 없다. 따라서 실제 맞는 정보의 양보다 더 많은 정보를 가질 수밖에 없다. 

 둘째, 예측된 분포가 실제 분포와 비슷해질 수록 크로스 엔트로피 값은 엔트로피 값과 가까워 진다. 

 





## 3.  머신러닝과 크로스 엔트로피





 머신 러닝을 통해 예측 모형을 만드는 것은 훈련 데이터를 바탕으로 새로운 데이터를 예측할 수 있는 확률 분포를 만드는 것이라고도 볼 수 있다.



 그렇기 때문에,

* 실제 확률 분포를 모르는 상태에서, 
* 학습 데이터를 바탕으로 미래의 확률 분포를 예측하기 위한 모델을 만들고, 
* 예측한 분포에 따라 획득한 정보가 얼마나 유용한지 **크로스 엔트로피**를 활용하여 그 정보량을 계산하면,

 머신러닝이 만들어 낸 모형의 정보량을 알 수 있다.



 그리고 모형을 만들어내는 학습 과정에서 손실 함수로서 크로스 엔트로피를 사용한다. 

  훈련 데이터에 대해서는 실제 분포 P를 알 수 있기 때문에, 예측 모델에서의 확률 분포 Q와 실제 분포 P 간의 크로스 엔트로피 값을 계산할 수 있다. 이렇게 계산된 크로스 엔트로피 값은 실제 값과 모델이 예측한 값 사이의 정보 불확실성을 나타낸다. 따라서 손실함수로서 크로스 엔트로피를 사용하여 **불확실성을 줄여 나가는 방향으로** 학습이 이루어지게 되는 것이다.

 

 특히 크로스 엔트로피의 성질을 상기시켜 본다면, 머신러닝의 학습이 잘 진행될수록 크로스 엔트로피 값과 엔트로피 값이 비슷해지게 되리라는 것을 알 수 있다. 따라서 크로스 엔트로피 값을 계산해서 학습 과정을 조정하면 머신러닝이 예측하는 확률 분포를 실제 확률 분포에 가깝게 만들 수 있는 것이다.



 기존에 배웠던 손실함수인 MSE와 비교해서 살펴 보면, 모델의 정확도가 상승할수록 CE와 MSE가 모두 감소하는 것을 알 수 있다. 

![image-20200623112106497]({{site.url}}/assets/images/image-20200623112106497.png)



 그리고 일반적으로 분류 문제에서는 MSE보다 CE를 사용하는 것이 더 좋다고 알려져 있다(이 부분에 대해서는 [여기](https://ratsgo.github.io/deep%20learning/2017/09/24/loss/)를 참고하자).

