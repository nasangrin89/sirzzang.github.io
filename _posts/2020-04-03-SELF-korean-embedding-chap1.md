---
title:  "[한국어 임베딩] 1. 서론"
toc: true
toc_sticky: true
header:
  teaser: /assets/images/blog-SELF.jpg
categories:
  - SELF
tags:
  - 한국어 임베딩
  - 자연어 처리
  - NLP
last_modified_at: 2020-04-03
---



# 1장. 서론



## 임베딩이란



> 컴퓨터는 어디까지나 빠르고 효율적인 "계산기"일 뿐이다. 한마디로 컴퓨터는 인간이 사용하는 자연어를 있는 그대로 이해하는 것이 아니라, 숫자를 계산한다는 이야기이다. 기계의 자연어 이해와 생성은 연산이나 처리의 영역이다.



 '임베딩'이란, '사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과, 혹은 그 일련의 과정 전체'를 의미한다. 단어나 문장 각각을 벡터로 변환해 벡터 공간으로 '끼워 넣는다'는 의미에서 임베딩이라고 부른다.

 임베딩이 필요한 이유는, 기계가 사람의 말을 이해할 수 있도록 하기 위해서이다. 위에서 저자가 설명하듯, 컴퓨터는 사람의 말을 '말'이 아니라, '숫자'로서 이해해야 한다. 그렇다면 컴퓨터로 사람의 말을 처리하기 위해서는, 컴퓨터가 사람의 말을 알아들을 수 있도록 숫자로 바꿔주는 과정이 필연적으로 선행되어야 한다.



## 임베딩의 역할



### 1. 단어-문장 간 관련도 계산



 **단어-문서 행렬**

|  구분  | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑 손님과 어머니 | 삼포 가는 길 |
| :----: | :------------: | :----------: | :----------------: | :----------: |
|  기차  |       0        |      2       |         10         |      7       |
| 막걸리 |       0        |      1       |         0          |      0       |
| 선술집 |       0        |      1       |         0          |      0       |

<center><sup>29페이지 단어-문서 행렬 예시</sup></center>

  형태소 분석기(*ex. Mecab, KoNLPy, ...*)를 통해 명사, 동사 등을 추출해 단어-문서 행렬을 구할 수 있다. 이를 통해 각 문서에서 사용하는 단어 목록 중 어떤 것이 겹치는지, 각 단어 간 의미 차이가 얼마나 큰지 등을 확인할 수 있다.



**유사도 계산**



 말뭉치를 형태소 분석한 뒤, 임베딩한다. 이렇게 임베딩한 결과를 바탕으로, 단어 벡터 간 **유사도**를 계산하여 **비슷한 단어들을 추출**할 수 있다. 형태소 분석기 Mecab을 활용해 네이버 영화 리뷰 말뭉치 등을 형태소 분석한 뒤, 벡터 간 코사인 유사도를 계산해 비슷한 단어들을 뽑아낸 결과는 다음과 같다.

| 희망   | 절망   | 학교     | 학생     | 가족   | 자동차     |
| ------ | ------ | -------- | -------- | ------ | ---------- |
| 소망   | 체념   | 초등     | 대학생   | 아이   | 승용차     |
| 행복   | 고뇌   | 중학교   | 대학원생 | 부모   | 상용차     |
| 희망찬 | 절망감 | 고등학교 | 고학생   | 편부모 | 트럭       |
| 꿈     | 상실감 | 야학교   | 교직원   | 고달픈 | 대형트럭   |
| 열망   | 번민   | 중학     | 학부모   | 사랑   | 모터사이클 |

<center><sup>31페이지 코사인 유사도 기준 상위 5개 단어 목록</sup></center>

 같은 방식을 사용해 코사인 유사도에 따라 단어들 간 관계를 **색깔(히트맵 이용)로 시각화**할 수도 있다.

 또한, 벡터 공간을 기하학적으로 시각화할 수도 있다. 형태소 분석하여 100차원으로 임베딩한 단어 벡터들을 차원 축소 기법을 사용해 2차원 벡터로 만든 뒤, 이를 좌표계에 나타내는 것이다.



 이와 같이, 임베딩을 통해 각 단어 간 유사도를 계산하고, 시각화하는 일이 가능하다.



### 2. 의미/문법 정보 함축



 (형태소 분석을 한 뒤 임베딩으로) 단어 벡터를 만들면, 단어는 숫자들의 모음인 벡터가 된다. 이제, 사칙연산이 가능해 진다. 그리고 각 단어 벡터 간 연산을 통해 의미적, 문법적 관계를 도출해낼 수도 있다. 예컨대, '아들 - 딸 _ 소녀 = 소년'이 그것이다. 

 사칙 연산을 통해 의미 차이를 알아낼 수 있다면, 다시 말해 의미/문법 차이가 임베딩 정보에 잘 함축되어 있다면 품질이 좋은 임베딩이다. 이렇게 단어 임베딩을 평가하는 방법을 **단어 유추 평가**라고 한다.



### 3. 전이 학습



 임베딩을 다른 딥러닝 모델의 입력값으로 활용할 수 있다. 이러한 기법을 **전이 학습**이라고 한다. 예컨대, 문서 내에 있는 단어들을 활용해 문서를 분류할 수 있다. 이 때 문서 내 단어들을 형태소 분석한 뒤, 임베딩하여 설정한 분류 모델의 입력값으로 넣는 것이다. **임베딩의 품질이 좋을수록, 문서 분류 모델의 결과가 좋을 것임은 자명하다.**

 조금 더 구체적으로 알아보자. 전이학습이라 함은, 이미 학습된 결과를 가지고 새로운 사실을 이해하는 것이다. 마치, 사람이 무언가를 배울 때 이미 알고 있는 지식을 활용하는 것과 같다. 앞에서 말했듯, 품질이 좋은 임베딩일 수록 임베딩을 통해 만들어진 단어 벡터에 의미, 문법 정보 등이 잘 함축되어 있다. 이 임베딩을 입력값으로 쓰는 모델은, 그 모델이 수행해야 하는 태스크를 **빠르고(= 모델의 train loss *수렴*이 빨라진다.), 정확하게(= 모델 태스크의 *정확도*가 올라간다.) 해낼 수 있을 것**이다.





## 임베딩 기법의 역사와 종류



 그렇다면 임베딩 기법은 어떤 흐름을 거쳐 발전해 왔을까? 흐름에 따라 그 기법의 종류를 정리해 보면 다음과 같다.



### 1. 통계 기반 → NN 기반



 초기 임베딩 기법은 대부분 말뭉치의 통계량을 직접적으로 활용했다. 



 **잠재 의미 분석**<sup>Latent Semantic Analysis</sup>

 통계 기반 임베딩 기법의 대표적 예이다. 단어 사용 빈도 등 말뭉치의 통계량 정보가 들어 있는 행렬에 **특이값 분해(Singular Value Decomposition)을 적용해 행렬에 속한 벡터의 차원을 축소**한다.

 잠재 의미 분석을 적용하는 가장 큰 이유는 계산량과 메모리 소비량을 줄이기 위함이다. 단어-문서 행렬을 예로 들어 보자. 말뭉치 전체의 어휘 수가 많기 때문에, 행렬의 행 수는 매우 크다. 그럼에도 불구하고 문서 하나에 모든 단어들이 쓰이는 경우는 드물기 때문에, 행렬 대부분의 값은 0이 된다. 이러한 희소 행렬을 모델의 입력값으로 쓴다고 해보자. 계산량, 메모리 소비량이 쓸데 없이 커지게 된다.

 이러한 경우 LSA를 적용하면, 큰 데이터를 단어 수준에서 혹은 문서 수준에서 소수의 단어, 소수의 주제로 표현할 수 있게 된다는 장점이 있다. 이렇게 잠재 의미 분석을 적용하여 차원을 축소할 수 있는 *대상*은 **단어-문서 행렬 외에도, TF-IDF 행렬, 단어-문맥 행렬, 점별 상호 정보량 행렬** 등이 있다.



**뉴럴 네트워크 기반 임베딩**

 신경망은 그 구조가 유연하고 표현력이 풍부하다는 장점이 있다. 따라서 자연어의 문맥을 상당 부분 학습할 수 있다.

 뉴럴 네트워크 기반 임베딩 기법은 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부분에 구멍을 뚫어 놓고 해당 단어가 무엇일지 맞추는 과정에서 학습된다.



### 2. 단어 수준 → 문장 수준



 **단어 수준**

 단어 임베딩 기법은 각각의 단어를 임베딩한다. 임베딩된 단어 벡터들이 **단어 수준**에서 문맥적 의미를 함축하게 되는 것이다. 대표적으로 *NPLM, Word2Vec, GloVe, FastText, Swivel* 등의 모델이 있다.

 하지만, 단어 수준의 임베딩 기법은 동음이의어를 분간하기 어렵다는 약점을 갖는다. 단어의 형태가 갖다면 동일한 단어로 취급하기 때문에, 모든 문맥 정보가 해당 단어 벡터에 투영된다. 자연히, 동음이의어가 많은 문서에서는 문맥 정보가 제대로 반영되기 어려워 진다.



**문장 수준**

 개별 단어가 아니라, **단어 시퀀스 전체**의 **문맥적 의미**를 함축하는 임베딩 기법이다. *ELMO, BERT, GPT* 등이 그것이다. 단어 임베딩 기법보다 전이 학습에서의 효과가 좋은 것으로 알려져 있다.



### 3. 룰 → 엔드투엔드 → 프리트레인/파인튜닝



**룰** 

 언어의 규칙<sup>rule</sup>을 모델에 알려준다. 즉, 언어학적인 지식을 활용하는 임베딩 기법이다.



**엔드투엔드 모델**

 자연어 처리 분야에서 딥러닝 기법을 적용한 것이다. 데이터를 통째로 모델에 넣고, 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 이해하도록 유도한다. 딥러닝 모델은 입력과 출력 사이의 관계를 잘 근사하기 때문에, 사람이 모델에 규칙을 직접 알려주지 않아도 된다는 장점이 있다.

 기계 번역에 널리 사용되었던 *시퀀스투시퀀스 모델*이 그 예이다.



**프리트레인과 파인튜닝**

 다운스트림과 업스트림 태스크로 나누어 프리트레인을 진행하고, 튜닝을 통해 모델의 성능을 향상시켜 나간다. 2018년 *ELMO* 모델이 자연된 이후 발전하고 있는 분야로, 비교적 최신의 기법이다.

 우리가 궁극적으로 풀어야 할 자연어 처리의 문제(예컨대, 품사 판별, 개체명 인식, 의미 분석 등)를 다운스트림 태스크라고 한다. 그리고 이에 앞서 해결되어야 할 선결 과제를 업스트림 태스크라고 한다. 단어/문장의 임베딩을 프리트레인하는 작업이 바로 업스트림 태스크에 해당한다.

 방식은 다음과 같다. 우선, 대규모 말뭉치로 임베딩을 만든다(프리트레인). 이후, 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만들고, 우리가 풀고 싶은 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트한다(파인 튜닝).

 자연어 처리에 있어 업스트림과 다운스트림은 뗄래야 뗄 수 없는 관계다. 임베딩 품질이 좋아야, 문제를 제대로 풀 수 있다. 업스트림의 품질이 좋을 수록, 다운스트림 문제를 잘 풀게 된다.



## 임베딩 기법의 종류와 성능



**행렬 분해 기반**

 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법이다. 행렬을 분해한 후, 둘 중 하나의 행렬만 쓰거나, 둘을 더하거나, 이어 붙여 임베딩으로 사용한다. 대표적인 모델로, *Glove, Swivel* 등이 있다. 



**예측 기반**

 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습이 이루어진다. 뉴럴 네트워크 기반 방법이 이에 해당한다.



**토픽 기반**

 주어진 문서의 잠재된 주제를 추론하는 방식으로 임베딩을 수행한다. *잠재 디리클레 할당<sup>Latent Dirichlet Allocation</sup>*이 대표적인 기법이다. 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환한다. ~~문서 수준의 임베딩(?)이라고 볼 수 있을까?~~



**임베딩 성능 평가**

 파인 튜닝 모델의 구조를 고정한 뒤 각각의 임베딩을 전이 학습시켜, 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등의 다운스트림 태스크 성능을 측정했다. 문장 수준 임베딩 기법인 *ELMo, GPT, BERT* 등이 단어 수준 임베딩 기법인 *Glove*를 크게 앞선다. 임베딩 품질이 각종 다운스트림 태스크 성능에 크게 영향을 주고 있음을 알 수 있따.





## 주요 용어



 앞으로 이 책은 텍스트 데이터를 다루도록 한다. 각각의 데이터를 일컫는 주요 용어는 다음과 같다.



### 말뭉치

 임베딩 학습이라는 특정 목적을 가지고 수집한 표본. 말뭉치 크기가 아무리 크더라도, 해당 데이터는 자연어의 일부밖에 커버하지 못한다.



### 컬렉션

 말뭉치에 속한 각각의 집합. 예컨대 한국어 위키 백과와 네이버 영화 리뷰를 말뭉치로 쓴다면, 이들 각각이 컬렉션이 된다.(*네이버 영화 리뷰 컬렉션*, *한국어 위키백과 컬렉션*)



### 문장

 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 독립적인 형식 단위. 실제로는 이렇게 엄밀하게 분리할 수 없으므로, 마침표나 느낌표, 물음표와 같은 구둣점을 활용해 구분된 문자열을 문장으로 취급한다. 텍스트 데이터의 기본 단위이다.



### 문서

 생각이나 감정, 정보를 공유하는 문장의 집합.



### 토큰

 단어, 형태소, 서브워드 등 문장을 구성하는 단위. 책에서 다룰 텍스트 데이터를 구성하는 최소 단위이다. 문장을 토큰 시퀀스로 분석하는 과정을 **토크나이즈**라고 하는데, 토큰 분리 기준은 그때그때 다를 수 있다.



**형태소 분석**

 문장을 토큰 중에서도 형태소 토큰으로 나누는 과정을 말한다. 별도의 설명이 없다면, 형태소 분석, 토크나이즈, 토큰화 등을 같은 의미로 받아들이자.

 다만, 한국어의 경우 대부분의 형태소 분석기가 품사 판별까지 함께 수행하므로, 형태소 분석을 '토큰화 + 품사 판별'로 이해하면 된다.



### 어휘 집합

 말뭉치에 있는 모든 문서를 문장으로 나누고, 토크나이즈를 실시한 후, 중복을 제거한 토큰들의 집합. 어휘 집합에 없는 토큰은 미등록 단어라고 한다.